{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-27T09:39:02.107240Z",
     "iopub.status.busy": "2025-09-27T09:39:02.106987Z",
     "iopub.status.idle": "2025-09-27T09:44:57.203267Z",
     "shell.execute_reply": "2025-09-27T09:44:57.202286Z",
     "shell.execute_reply.started": "2025-09-27T09:39:02.107218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# vLLM itself\n",
    "!pip install vllm\n",
    "\n",
    "# Hugging Face transformers for model handling\n",
    "!pip install transformers\n",
    "\n",
    "# BitsAndBytes (for quantization support like 4-bit/8-bit)\n",
    "!pip install bitsandbytes\n",
    "\n",
    "# Accelerate (required for model device mapping)\n",
    "!pip install accelerate\n",
    "# vLLM itself\n",
    "!pip install vllm\n",
    "\n",
    "# Hugging Face transformers for model handling\n",
    "!pip install transformers\n",
    "\n",
    "# BitsAndBytes (for quantization support like 4-bit/8-bit)\n",
    "!pip install bitsandbytes\n",
    "\n",
    "# Accelerate (required for model device mapping)\n",
    "!pip install accelerate\n",
    "\n",
    "# For caching models and logging\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T09:44:57.205034Z",
     "iopub.status.busy": "2025-09-27T09:44:57.204773Z",
     "iopub.status.idle": "2025-09-27T09:44:59.215368Z",
     "shell.execute_reply": "2025-09-27T09:44:59.214595Z",
     "shell.execute_reply.started": "2025-09-27T09:44:57.205011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check GPU status\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"Available Memory: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9:.2f} GB\")\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T09:44:59.216594Z",
     "iopub.status.busy": "2025-09-27T09:44:59.216270Z",
     "iopub.status.idle": "2025-09-27T10:13:43.180442Z",
     "shell.execute_reply": "2025-09-27T10:13:43.179670Z",
     "shell.execute_reply.started": "2025-09-27T09:44:59.216575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['VLLM_USE_V1'] = '0'  # Disable V1 engine\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Try to free all CUDA memory\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        with torch.cuda.device(i):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            \n",
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv('/kaggle/input/banglachq/train.csv')\n",
    "print(f\"Loaded {len(df)} rows from train.csv\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Load model with vLLM on single GPU with optimized settings\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "print(f\"\\nLoading {model_name} with vLLM...\")\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"float16\",\n",
    "    gpu_memory_utilization=0.65,  # Reduced slightly for larger context\n",
    "    max_model_len=2048,  # Increased to 2048\n",
    "    max_num_seqs=24,  # Reduced batch size slightly\n",
    ")\n",
    "\n",
    "# Get tokenizer for chat template\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "print(\"Model loaded successfully!\\n\")\n",
    "\n",
    "# Translation function\n",
    "def translate_to_chittagonian(bangla_text):\n",
    "    prompt = f\"\"\"You are a precise translation tool. Your only task is to translate the given Bangla text to Chittagonian (Chatgaiyan) dialect using Bengali script.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Translate the Bangla text below to Chittagonian (Chatgaiyan) dialect\n",
    "- Use only Bengali script (not Latin script or IPA)\n",
    "- Return ONLY the translated text with no additional commentary, explanations, or notes\n",
    "- Do not include phrases like \"Here is the translation:\" or \"The Chittagonian translation is:\"\n",
    "- Do not add any metadata, formatting, or extra information\n",
    "- If you cannot translate a specific word, keep it as is in the original form\n",
    "\n",
    "Bangla text to translate:\n",
    "{bangla_text}\n",
    "\n",
    "Chittagonian translation:\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Apply chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    return formatted_prompt\n",
    "\n",
    "# Set sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.3,\n",
    "    top_p=0.9,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "# Process in smaller batches to avoid OOM\n",
    "BATCH_SIZE = 32  # Match max_num_seqs\n",
    "all_translations = []\n",
    "\n",
    "print(f\"Processing {len(df)} sentences in batches of {BATCH_SIZE}...\")\n",
    "\n",
    "for batch_start in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, len(df))\n",
    "    batch_df = df.iloc[batch_start:batch_end]\n",
    "    \n",
    "    # Prepare prompts for this batch\n",
    "    batch_prompts = []\n",
    "    for idx, row in batch_df.iterrows():\n",
    "        bangla_text = row['question']\n",
    "        prompt = translate_to_chittagonian(bangla_text)\n",
    "        batch_prompts.append(prompt)\n",
    "    \n",
    "    # Generate translations for this batch\n",
    "    outputs = llm.generate(batch_prompts, sampling_params)\n",
    "    \n",
    "    # Extract translations\n",
    "    batch_translations = [output.outputs[0].text.strip() for output in outputs]\n",
    "    all_translations.extend(batch_translations)\n",
    "    \n",
    "    # Clear memory after each batch\n",
    "    del outputs, batch_prompts\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Print first few examples\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for idx in range(min(3, len(df))):\n",
    "    print(f\"\\nBangla: {df.iloc[idx]['question']}\")\n",
    "    print(f\"Chittagonian: {all_translations[idx]}\")\n",
    "\n",
    "# Add translations to dataframe\n",
    "df['chittagonian_translation'] = all_translations\n",
    "\n",
    "# Save results\n",
    "df.to_csv('qwen_5_train_with_translations.csv', index=False)\n",
    "print(f\"\\nâœ… Translation complete! Saved to 'qwen_10_train_with_translations.csv'\")\n",
    "print(f\"Total sentences translated: {len(df)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8356525,
     "sourceId": 13186700,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
